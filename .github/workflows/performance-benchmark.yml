name: Android Performance Benchmark

on:
  workflow_dispatch:
  pull_request:
    paths:
      - "app/**"
      - "benchmark/**"
      - "baselineprofile/**"
      - "scripts/perf/**"
      - "docs/performance/**"
  push:
    branches:
      - develop
      - release/**
    paths:
      - "app/**"
      - "benchmark/**"
      - "baselineprofile/**"
      - "scripts/perf/**"
      - "docs/performance/**"

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: macos-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate Gradle wrapper
        uses: gradle/actions/wrapper-validation@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "17"

      - name: Gradle cache
        uses: gradle/actions/setup-gradle@v4

      - name: Run baseline profile + macrobenchmark on emulator
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          profile: pixel_7
          disable-animations: true
          script: |
            ./gradlew :baselineprofile:connectedBenchmarkAndroidTest
            ./gradlew :benchmark:connectedBenchmarkAndroidTest

      - name: Collect benchmark artifacts
        if: always()
        run: |
          mkdir -p perf-artifacts
          cp -R benchmark/build/outputs perf-artifacts/benchmark-outputs || true
          cp -R baselineprofile/build/outputs perf-artifacts/baselineprofile-outputs || true
          BENCH_JSON="$(find benchmark/build/outputs/connected_android_test_additional_output -name '*benchmarkData.json' | head -n 1)"
          if [ -n "$BENCH_JSON" ]; then
            python3 scripts/perf/extract_current_metrics.py \
              --input "$BENCH_JSON" \
              --output perf-artifacts/benchmark-outputs/current_metrics.json
          else
            echo "benchmarkData.json not found - cannot generate current_metrics.json"
          fi

      - name: Evaluate regression gate (if current metrics json exists)
        if: always()
        run: |
          if [ -f perf-artifacts/benchmark-outputs/current_metrics.json ]; then
            python3 scripts/perf/perf_gate.py \
              --baseline Docs/performance/perf_gate_baseline.json \
              --current perf-artifacts/benchmark-outputs/current_metrics.json \
              --report perf-artifacts/perf_report.md
          else
            echo "current_metrics.json not found - skipping gate evaluation"
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: android-performance-benchmark
          path: perf-artifacts
